{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xuanyu410/114-1PL-Repo/blob/main/%E7%A8%8B%E5%BC%8F%E8%AA%9E%E8%A8%80%E4%BD%9C%E6%A5%AD%E5%9B%9B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# æ­¥é©Ÿ 1: å®‰è£ä¾è³´ä¸¦è¼‰å…¥ API Key\n",
        "!pip install -q gspread pandas requests beautifulsoup4 jieba scikit-learn gradio\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# ğŸš¨ ä¿®æ­£ï¼šå°‡æ‚¨åœ¨ Colab Secret ä¸­è¨­ç½®çš„ 'int' è¼‰å…¥åˆ°æ¨™æº–ç’°å¢ƒè®Šæ•¸ 'GEMINI_API_KEY' ä¸­\n",
        "# ç¢ºä¿å¾ŒçºŒæ‰€æœ‰ç¨‹å¼ç¢¼éƒ½ä½¿ç”¨é€™å€‹æ¨™æº–åç¨±\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"int\")\n",
        "\n",
        "# æª¢æŸ¥æ˜¯å¦æˆåŠŸè¼‰å…¥\n",
        "api_key_prefix = os.environ[\"GEMINI_API_KEY\"][:5] if os.environ.get(\"GEMINI_API_KEY\") else \"\"\n",
        "\n",
        "if api_key_prefix:\n",
        "    print(f\"âœ… Gemini API Key è¼‰å…¥æˆåŠŸï¼(Prefix: {api_key_prefix}...)\")\n",
        "else:\n",
        "    # å¦‚æœæ‚¨çœ‹åˆ°é€™å€‹è­¦å‘Šï¼Œä»£è¡¨æ‚¨çš„ Secret 'int' å¯èƒ½æ²’æœ‰è¨­ç½®æˆ–å€¼æ˜¯ç©ºçš„\n",
        "    print(\"âŒ è­¦å‘Šï¼šè«‹ç¢ºèªæ‚¨å·²åœ¨ Colab Secrets ä¸­è¨­ç½® 'int' ä¸¦æˆåŠŸè¼‰å…¥ï¼\")\n",
        "\n",
        "# åŸ·è¡Œ Google Sheet èªè­‰ (ä½¿ç”¨æ‚¨è‡ªå·±çš„ Google å¸³è™Ÿ)\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "print(\"âœ… Google Colab èªè­‰æˆåŠŸï¼\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw8VohUafVuz",
        "outputId": "680b3950-8fe2-401b-fc25-ecb4792f6fd2"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Gemini API Key è¼‰å…¥æˆåŠŸï¼(Prefix: AIzaS...)\n",
            "âœ… Google Colab èªè­‰æˆåŠŸï¼\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import jieba\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import gradio as gr\n",
        "\n",
        "# Google Sheet ç›¸é—œå¥—ä»¶\n",
        "import gspread\n",
        "from google.auth import default\n",
        "\n",
        "# --- é…ç½®å€å¡Š ---\n",
        "PTT_BOARD = \"KoreaStar\"\n",
        "BASE_URL = f\"https://www.ptt.cc/bbs/{PTT_BOARD}/\"\n",
        "HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "\n",
        "# Gemini API é…ç½®ï¼šå¾ç’°å¢ƒè®Šé‡ä¸­ç²å– API Key\n",
        "API_KEY = os.environ.get(\"GEMINI_API_KEY\") # ç¢ºä¿ä½¿ç”¨ä¿®æ­£å¾Œçš„è®Šæ•¸åç¨±\n",
        "\n",
        "# æª¢æŸ¥ API Key æ˜¯å¦å­˜åœ¨\n",
        "display_key = API_KEY if API_KEY else \"INVALID_OR_MISSING_KEY\"\n",
        "print(f\"Gemini API URL ä½¿ç”¨çš„ Key å‰ç¶´: {display_key[:5]}...\")\n",
        "\n",
        "if API_KEY:\n",
        "    GEMINI_API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key={API_KEY}\"\n",
        "else:\n",
        "    GEMINI_API_URL = \"\"\n",
        "    print(\"âŒ è­¦å‘Šï¼šGEMINI_API_KEY éºå¤±æˆ–ç„¡æ•ˆï¼API å‘¼å«å°‡å¤±æ•—ã€‚\")\n",
        "\n",
        "\n",
        "# --- Google Sheet é…ç½® ---\n",
        "SHEET_URL = \"https://docs.google.com/spreadsheets/d/1UIfts0iHJzLn6VdOeuT3WS7UKDEdS5dylr9WxK1BhFA/edit?gid=579218704#gid=579218704\"\n",
        "WORKSHEET_NAME = \"çˆ¬èŸ²çµæœ\"\n",
        "# åˆ†æçµæœè¦å¯«å…¥çš„åˆ†é åç¨±\n",
        "WORKSHEET_NAME_ANALYSIS = \"kpopç†±è©èˆ‡aiåˆ†æ\"\n",
        "\n",
        "# åœç”¨è©åˆ—è¡¨\n",
        "STOPWORDS = set(['çš„','äº†','æ˜¯','ws','www','com','https','http','we','åœ¨', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'ä¹‹', 'ä¸€å€‹', 'å’Œ', 'è¨è«–', 'åˆ†äº«', 'å ±å°', 'æ–°è', 'å¨›æ¨‚', 'è¨˜è€…', 'ç¶œåˆ', 'ç™¼ä¿¡', 'ç«™å…§', 'æ¨™é¡Œ', 'æ™‚é–“', 'ä¾†æº', 'ptt', 'ç™¼æ–‡', 'ä½œè€…', 'æ²’æœ‰', 'æœ€æ–°', 'çœŸçš„', 'ä»€éº¼', 'å¤§å®¶', 'å°±æ˜¯', 'é€™å€‹', 'é‚£å€‹', 'å¯ä»¥' ])\n",
        "\n",
        "# --- Google Sheet é€£æ¥å‡½æ•¸ ---\n",
        "def gsheet_connect_colab(worksheet_name):\n",
        "    \"\"\"ä½¿ç”¨ Colab èªè­‰é€£æ¥ Google Sheetï¼Œä¸¦æ ¹æ“šå‚³å…¥çš„åç¨±é–‹å•Ÿæˆ–å‰µå»ºå·¥ä½œè¡¨\"\"\"\n",
        "    try:\n",
        "        creds, _ = default()\n",
        "        gc = gspread.authorize(creds)\n",
        "        sh = gc.open_by_url(SHEET_URL)\n",
        "\n",
        "        try:\n",
        "            # å˜—è©¦ç²å–å·¥ä½œè¡¨\n",
        "            worksheet = sh.worksheet(worksheet_name)\n",
        "        except gspread.WorksheetNotFound:\n",
        "            # å‰µå»ºæ–°çš„å·¥ä½œè¡¨\n",
        "            worksheet = sh.add_worksheet(title=worksheet_name, rows=\"1000\", cols=\"5\")\n",
        "\n",
        "        return worksheet, \"GSHEET_COLAB\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Google Sheet é€£æ¥æˆ–èªè­‰å¤±æ•—: {worksheet_name} ({e})\")\n",
        "        return None, \"ERROR\"\n",
        "\n",
        "\n",
        "# --- 1. çˆ¬èŸ²æ ¸å¿ƒå‡½æ•¸ ---\n",
        "\n",
        "def get_latest_index(board_url):\n",
        "    \"\"\"ç²å–æœ€æ–°æ–‡ç« é é¢çš„ä¸Šä¸€é  (å³ç¬¬ä¸€é çš„é ç¢¼)\"\"\"\n",
        "    try:\n",
        "        response = requests.get(board_url, headers=HEADERS, timeout=5)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        paging_div = soup.find('div', class_='btn-group btn-group-paging')\n",
        "        if paging_div:\n",
        "            prev_button = paging_div.find_all('a')[1]\n",
        "            if 'href' in prev_button.attrs:\n",
        "                match = re.search(r'index(\\d+)\\.html', prev_button['href'])\n",
        "                if match:\n",
        "                    # è¿”å›ç•¶å‰æœ€æ–°é çš„é ç¢¼\n",
        "                    return int(match.group(1)) + 1\n",
        "        return 1\n",
        "    except Exception:\n",
        "        return 1\n",
        "\n",
        "def get_article_content(article_href):\n",
        "    \"\"\"ç²å–å–®ç¯‡æ–‡ç« çš„å…§æ–‡\"\"\"\n",
        "    if article_href.startswith('N/A'):\n",
        "        return \"æ–‡ç« å·²è¢«åˆªé™¤æˆ–ä¸å¯å­˜å–ã€‚\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(article_href, headers=HEADERS, timeout=5)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        main_content = soup.find(id=\"main-content\")\n",
        "\n",
        "        if not main_content:\n",
        "            return \"ç„¡æ³•æ‰¾åˆ°æ–‡ç« å…§å®¹ã€‚\"\n",
        "\n",
        "        # æ¸…ç† Meta è³‡è¨Šã€æ¨æ–‡å’Œé åº•è³‡è¨Š\n",
        "        for tag_class in ['article-metaline', 'article-metaline-right', 'push', 'f2', 'f6']:\n",
        "             for tag in main_content.find_all('div', class_=tag_class):\n",
        "                tag.extract()\n",
        "\n",
        "        content = main_content.text.strip()\n",
        "        content = re.sub(r'(â€» ç™¼ä¿¡ç«™: .*|â€» æ–‡ç« ç¶²å€: .*|ä½œè€…: .*|\\n)', '', content).strip()\n",
        "        return content\n",
        "\n",
        "    except Exception:\n",
        "        return \"ç„¡æ³•æŠ“å–å…§æ–‡ã€‚\"\n",
        "\n",
        "\n",
        "def crawl_ptt_koreastar(pages_to_fetch):\n",
        "    \"\"\"ä¸»çˆ¬èŸ²å‡½æ•¸ï¼šçˆ¬å–å¤šé æ–‡ç« åˆ—è¡¨ä¸¦æŠ“å–å…§æ–‡ï¼Œä¸¦å¯«å…¥ Google Sheet\"\"\"\n",
        "\n",
        "    start_index = get_latest_index(BASE_URL)\n",
        "    stop_index = max(1, start_index - pages_to_fetch)\n",
        "    articles_data = []\n",
        "\n",
        "    for index in range(start_index, stop_index, -1):\n",
        "        url = f\"{BASE_URL}index{index}.html\"\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=HEADERS, timeout=5)\n",
        "            if response.status_code != 200:\n",
        "                 continue\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            article_list = soup.find_all('div', class_='r-ent')\n",
        "\n",
        "            for article in article_list:\n",
        "                title_tag = article.find('div', class_='title').find('a')\n",
        "\n",
        "                if title_tag:\n",
        "                    title = title_tag.text.strip()\n",
        "                    href = \"https://www.ptt.cc\" + title_tag['href']\n",
        "                    content = get_article_content(href)\n",
        "                    author = article.find('div', class_='author').text.strip()\n",
        "                    date = article.find('div', class_='date').text.strip()\n",
        "\n",
        "                    articles_data.append({\n",
        "                        'title': title,\n",
        "                        'date': date,\n",
        "                        'author': author,\n",
        "                        'href': href,\n",
        "                        'content': content\n",
        "                    })\n",
        "\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    df = pd.DataFrame(articles_data)\n",
        "    df = df[df['content'].str.strip() != 'æ–‡ç« å·²è¢«åˆªé™¤æˆ–ä¸å¯å­˜å–ã€‚'].reset_index(drop=True)\n",
        "\n",
        "    if df.empty:\n",
        "        return df, \"âŒ çˆ¬å–å®Œæˆï¼Œä½†æœªæ‰¾åˆ°æœ‰æ•ˆæ–‡ç« ã€‚\"\n",
        "\n",
        "    # æ­¥é©Ÿ 3: å¯«å…¥ Google Sheet (å·¥ä½œè¡¨4)\n",
        "    worksheet, connection_type = gsheet_connect_colab(WORKSHEET_NAME)\n",
        "\n",
        "    if connection_type == \"GSHEET_COLAB\" and worksheet:\n",
        "        try:\n",
        "            # æ¸…é™¤ç¾æœ‰å…§å®¹ (ä¿ç•™æ¨™é ­)\n",
        "            worksheet.clear()\n",
        "\n",
        "            # æº–å‚™æ•¸æ“šï¼ŒåŒ…å«æ¨™é ­\n",
        "            cols_order = ['title', 'date', 'author', 'href', 'content']\n",
        "            df = df[cols_order]\n",
        "            data_to_write = [df.columns.values.tolist()] + df.values.tolist()\n",
        "\n",
        "            # å¯«å…¥æ•¸æ“š\n",
        "            worksheet.update(data_to_write, value_input_option='USER_ENTERED')\n",
        "\n",
        "            status_msg = f\"âœ… æˆåŠŸçˆ¬å– {len(df)} ç¯‡æ–‡ç« ä¸¦**å¯«å…¥ Google Sheet åˆ†é : {WORKSHEET_NAME}**\"\n",
        "\n",
        "        except Exception as e:\n",
        "            status_msg = f\"âš ï¸ Google Sheet å¯«å…¥å¤±æ•— ({e})ã€‚è«‹ç¢ºèªæ‚¨æ˜¯å¦å·²åŸ·è¡Œ `auth.authenticate_user()` ä¸”æ“æœ‰ç·¨è¼¯æ¬Šé™ã€‚\"\n",
        "\n",
        "    else:\n",
        "        status_msg = \"âŒ Google Sheet é€£æ¥å¤±æ•—ï¼Œç„¡æ³•å¯«å…¥ã€‚è«‹ç¢ºèª Colab èªè­‰æ˜¯å¦æˆåŠŸã€‚\"\n",
        "\n",
        "    return df, status_msg\n",
        "\n",
        "\n",
        "# --- 2. æ•¸æ“šè®€å–èˆ‡æ–‡æœ¬åˆ†ææ ¸å¿ƒå‡½æ•¸ ---\n",
        "\n",
        "def analyze_keywords(df, top_n=10):\n",
        "    \"\"\"åŸ·è¡Œ TF-IDF é—œéµå­—çµ±è¨ˆ\"\"\"\n",
        "    if df.empty:\n",
        "        return pd.DataFrame(), \"éŒ¯èª¤: è³‡æ–™é›†ç‚ºç©ºï¼Œç„¡æ³•é€²è¡Œåˆ†æã€‚\"\n",
        "\n",
        "    documents = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        # æ¨™é¡Œæ¬Šé‡ * 3\n",
        "        text = row['title'] * 3 + \" \" + row['content']\n",
        "        cleaned_text = re.sub(r'[^\\\\u4e00-\\\\u9fff\\\\w]', ' ', text)\n",
        "        words = jieba.lcut(cleaned_text, cut_all=False)\n",
        "\n",
        "        filtered_words = [\n",
        "            word.strip().lower()\n",
        "            for word in words\n",
        "            if word.strip() and len(word.strip()) > 1 and word.strip().lower() not in STOPWORDS and not word.isdigit()\n",
        "        ]\n",
        "        documents.append(\" \".join(filtered_words))\n",
        "\n",
        "    if not any(documents):\n",
        "         return pd.DataFrame(), \"éŒ¯èª¤: æ•¸æ“šæ¸…ç†å¾Œç„¡æœ‰æ•ˆè©å½™é€²è¡Œ TF-IDF åˆ†æã€‚\"\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    tfidf_array = tfidf_matrix.toarray()\n",
        "\n",
        "    # è¨ˆç®—è©å½™çš„ TF-IDF å¹³å‡æ¬Šé‡\n",
        "    avg_tfidf_scores = defaultdict(float)\n",
        "    num_documents = len(documents)\n",
        "\n",
        "    for doc_weights in tfidf_array:\n",
        "        for i, weight in enumerate(doc_weights):\n",
        "            avg_tfidf_scores[feature_names[i]] += weight\n",
        "\n",
        "    for word in avg_tfidf_scores:\n",
        "        avg_tfidf_scores[word] /= num_documents\n",
        "\n",
        "    sorted_avg_tfidf = sorted(avg_tfidf_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    top_n_keywords = sorted_avg_tfidf[:top_n]\n",
        "\n",
        "    keyword_df = pd.DataFrame(top_n_keywords, columns=['é—œéµå­— (Top N ç†±è©)', 'TF-IDF å¹³å‡æ¬Šé‡'])\n",
        "    keyword_df['TF-IDF å¹³å‡æ¬Šé‡'] = keyword_df['TF-IDF å¹³å‡æ¬Šé‡'].round(4)\n",
        "\n",
        "    return keyword_df, f\"âœ… TF-IDF é—œéµå­—çµ±è¨ˆæˆåŠŸï¼Œè¼¸å‡º Top {top_n} ç†±è©ã€‚\"\n",
        "\n",
        "\n",
        "# --- 3. Gemini API ä¸²æ¥æ ¸å¿ƒå‡½æ•¸ (ä¿æŒä¸è®Š) ---\n",
        "\n",
        "def generate_summary(keyword_df, df_articles, user_question):\n",
        "    \"\"\"ä¸²æ¥ Gemini API ç”Ÿæˆæ´å¯Ÿæ‘˜è¦èˆ‡çµè«–\"\"\"\n",
        "    if keyword_df.empty or df_articles.empty:\n",
        "        return \"ç„¡æ³•ç”Ÿæˆæ‘˜è¦ã€‚è«‹å…ˆåŸ·è¡Œçˆ¬èŸ²èˆ‡åˆ†ææ­¥é©Ÿã€‚\", \"\"\n",
        "\n",
        "    # å¦‚æœ API Key éºå¤±ï¼Œç›´æ¥è¿”å›éŒ¯èª¤\n",
        "    if not API_KEY:\n",
        "        return \"âŒ éŒ¯èª¤ï¼šGemini API Key å°šæœªè¨­ç½®æˆ–ç„¡æ•ˆã€‚\", \"\"\n",
        "\n",
        "\n",
        "    top_keywords = \", \".join(keyword_df['é—œéµå­— (Top N ç†±è©)'].tolist())\n",
        "    article_titles = \"\\n- \" + \"\\n- \".join(df_articles['title'].head(30).tolist())\n",
        "\n",
        "    system_prompt = (\n",
        "        \"ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„éŸ“åœ‹å¨›æ¨‚åœˆè¶¨å‹¢åˆ†æå¸«ã€‚ä½ çš„ä»»å‹™æ˜¯åŸºæ–¼æä¾›çš„ PTT éŸ“æ˜Ÿç‰ˆæ•¸æ“šï¼Œ\"\n",
        "        \"åˆ†ææœ€è¿‘çš„è¨è«–ç†±é»ã€è¶¨å‹¢ã€ä»¥åŠä½¿ç”¨è€…æœ€é—œå¿ƒçš„å•é¡Œï¼ˆå¦‚æœæä¾›çš„è©±ï¼‰ã€‚\"\n",
        "        \"å›ç­”å¿…é ˆæ˜¯ç¹é«”ä¸­æ–‡ï¼Œä¸”æ ¼å¼éœ€åš´æ ¼éµå¾ªè¦æ±‚ã€‚\"\n",
        "    )\n",
        "\n",
        "    user_prompt = (\n",
        "        \"è«‹æ ¹æ“šä»¥ä¸‹æ•¸æ“šç”Ÿæˆåˆ†æçµæœï¼š\\n\\n\"\n",
        "        \"1. **ç†±é–€é—œéµå­— (TF-IDF Top 10):**\\n\"\n",
        "        f\"{top_keywords}\\n\\n\"\n",
        "        \"2. **æœ€æ–°ç†±é–€æ–‡ç« æ¨™é¡Œ (æœ€è¿‘30ç¯‡):**\\n\"\n",
        "        f\"{article_titles}\\n\\n\"\n",
        "        \"3. **ä½¿ç”¨è€…æ„Ÿèˆˆè¶£çš„å•é¡Œ:**\\n\"\n",
        "        f\"'{user_question}'\\n\\n\"\n",
        "        \"åˆ†æè¦æ±‚:\\n\"\n",
        "        \"A. ç”Ÿæˆ 5 å¥**å…·é«”çš„**æ´å¯Ÿæ‘˜è¦ (Insights)ï¼Œæ¯å¥å–®ç¨ä»¥æ˜Ÿè™Ÿé–‹é ­ï¼Œå–®ç¨æˆè¡Œã€‚\\n\"\n",
        "        \"B. ç”Ÿæˆä¸€æ®µç´„ 120 å­—çš„çµè«–ï¼Œç¸½çµéŸ“åœ‹å¨›æ¨‚åœˆè¿‘æœŸåœ¨ç´…ä»€éº¼ã€‚\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            GEMINI_API_URL,\n",
        "            headers={'Content-Type': 'application/json'},\n",
        "            data=json.dumps({\n",
        "                \"contents\": [{\"parts\": [{\"text\": user_prompt}]}],\n",
        "                \"systemInstruction\": {\"parts\": [{\"text\": system_prompt}]},\\\n",
        "            }),\n",
        "            timeout=60\n",
        "        )\n",
        "\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "\n",
        "        generated_text = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', 'API è¿”å›ç©ºçµæœã€‚')\n",
        "\n",
        "        lines = generated_text.split('\\n')\n",
        "        insights = []\n",
        "        conclusion_parts = []\n",
        "\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            if stripped_line.startswith('*') or re.match(r'^[\\d]\\. ', stripped_line):\n",
        "                insights.append(stripped_line.lstrip('* ').lstrip('12345. '))\n",
        "            elif stripped_line:\n",
        "                conclusion_parts.append(stripped_line)\n",
        "\n",
        "        insight_output = \"* \" + \"\\n* \".join(insights[:5])\n",
        "        conclusion_output = \" \".join(conclusion_parts).strip()\n",
        "\n",
        "        if not insight_output.strip() or not conclusion_output.strip():\n",
        "            return \"* (è§£æå¤±æ•—ï¼ŒåŸå§‹è¼¸å‡ºå¦‚ä¸‹:)\\n\" + generated_text, \"(è§£æå¤±æ•—ï¼Œè«‹è¦‹æ‘˜è¦å€)\"\n",
        "\n",
        "        return insight_output, conclusion_output\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        error_msg = f\"âŒ Gemini API å‘¼å«å¤±æ•—: {e}ã€‚è«‹æª¢æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆæˆ–æ˜¯å¦æœ‰è¶³å¤ çš„é¤˜é¡ã€‚\"\n",
        "        return error_msg, \"\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        error_msg = f\"âŒ Gemini API å‘¼å«å¤±æ•—: {e}\"\n",
        "        return error_msg, \"\"\n",
        "    except Exception as e:\n",
        "        return f\"è™•ç† Gemini éŸ¿æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\", \"\"\n",
        "# --- 4. åˆ†æçµæœå¯«å…¥ Google Sheet å‡½æ•¸ (ç¬¦åˆæ‚¨çš„æ¨™é ­è¦æ±‚) ---\n",
        "\n",
        "def write_analysis_to_gsheet(keyword_df, insight_output, conclusion_output):\n",
        "    \"\"\"å°‡ TF-IDF ç†±è©ã€AI æ´å¯Ÿèˆ‡çµè«–å¯«å…¥æŒ‡å®šçš„ Google Sheet åˆ†é \"\"\"\n",
        "\n",
        "    # ä½¿ç”¨ WORKSHEET_NAME_ANALYSIS é€£æ¥æ–°åˆ†é \n",
        "    worksheet, connection_type = gsheet_connect_colab(WORKSHEET_NAME_ANALYSIS)\n",
        "\n",
        "    if connection_type != \"GSHEET_COLAB\" or not worksheet:\n",
        "        return f\"âŒ åˆ†æçµæœ Google Sheet é€£æ¥å¤±æ•—: {WORKSHEET_NAME_ANALYSIS}\"\n",
        "\n",
        "    try:\n",
        "        # 1. æ¸…é™¤ç¾æœ‰å…§å®¹\n",
        "        worksheet.clear()\n",
        "\n",
        "        # 2. æº–å‚™æ•¸æ“š\n",
        "\n",
        "        # æå–é—œéµå­—åˆ—è¡¨\n",
        "        keywords = keyword_df['é—œéµå­— (Top N ç†±è©)'].tolist()\n",
        "\n",
        "        # æå–æ´å¯Ÿæ‘˜è¦åˆ—è¡¨\n",
        "        insights = [\n",
        "            line.strip().lstrip('* ').lstrip('12345. ')\n",
        "            for line in insight_output.split('\\n') if line.strip().startswith('*') or re.match(r'^[\\d]\\. ', line.strip())\n",
        "        ][:5] # åªå–å‰ 5 å¥\n",
        "\n",
        "        # ç¢ºå®šè¦å¯«å…¥çš„æœ€å¤§è¡Œæ•¸ï¼ˆè‡³å°‘æ˜¯ Insights çš„æ•¸é‡ï¼‰\n",
        "        max_rows = max(len(keywords), 5)\n",
        "\n",
        "        # æº–å‚™è¦å¯«å…¥çš„æ•¸æ“šçŸ©é™£\n",
        "        data_to_write = []\n",
        "\n",
        "        # è¨­å®šæ¨™é ­ï¼š[\"ç†±è©\", \"5 å¥æ´å¯Ÿæ‘˜è¦\", \"çµè«–\"]\n",
        "        headers = [\"ç†±è©\", \"5 å¥æ´å¯Ÿæ‘˜è¦\", \"çµè«–\"]\n",
        "        data_to_write.append(headers)\n",
        "\n",
        "        # éæ­·æœ€å¤§è¡Œæ•¸ï¼Œæ§‹å»ºæ•¸æ“šè¡Œ\n",
        "        for i in range(max_rows):\n",
        "            row = []\n",
        "\n",
        "            # ç†±è© (Column 1)\n",
        "            row.append(keywords[i] if i < len(keywords) else \"\")\n",
        "\n",
        "            # æ´å¯Ÿæ‘˜è¦ (Column 2)\n",
        "            row.append(insights[i] if i < len(insights) else \"\")\n",
        "\n",
        "            # çµè«– (Column 3) - åªåœ¨ç¬¬ä¸€è¡Œå¯«å…¥\n",
        "            row.append(conclusion_output if i == 0 else \"\")\n",
        "\n",
        "            data_to_write.append(row)\n",
        "\n",
        "        # 3. å¯«å…¥æ•¸æ“š\n",
        "        worksheet.update(data_to_write, value_input_option='USER_ENTERED')\n",
        "\n",
        "        return f\"âœ… æˆåŠŸå°‡ {len(keywords)} ç­†ç†±è©åŠ AI åˆ†æçµæœå¯«å…¥ Google Sheet åˆ†é : {WORKSHEET_NAME_ANALYSIS}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ åˆ†æçµæœ Google Sheet å¯«å…¥å¤±æ•— ({e})ã€‚è«‹ç¢ºèªç·¨è¼¯æ¬Šé™ã€‚\""
      ],
      "metadata": {
        "id": "2ZciWrK4mhwl",
        "outputId": "6e930698-fae7-4892-afd2-7d0ab2b34db9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API URL ä½¿ç”¨çš„ Key å‰ç¶´: AIzaS...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Gradio ä»‹é¢æµç¨‹å‡½æ•¸ ---\n",
        "\n",
        "def full_analysis_workflow(pages_to_fetch, user_question, top_n=10):\n",
        "    \"\"\"ä¸€éµåŸ·è¡Œçˆ¬èŸ²ã€åˆ†æã€ç¸½çµçš„å®Œæ•´æµç¨‹\"\"\"\n",
        "    if pages_to_fetch <= 0:\n",
        "        # ğŸš¨ ä¿®æ­£ï¼šå°‡ \"âŒ éŒ¯èª¤: çˆ¬å–é æ•¸å¿…é ˆå¤§æ–¼ 0ã€‚\"\" æ”¹ç‚º \"âŒ éŒ¯èª¤: çˆ¬å–é æ•¸å¿…é ˆå¤§æ–¼ 0ã€‚\"\n",
        "        return (pd.DataFrame(), \"âŒ éŒ¯èª¤: çˆ¬å–é æ•¸å¿…é ˆå¤§æ–¼ 0ã€‚\", \"åˆ†æå¤±æ•—\", \"åˆ†æå¤±æ•—\", \"åˆ†æå¤±æ•—\")\n",
        "\n",
        "    # æ­¥é©Ÿ 1: åŸ·è¡Œçˆ¬èŸ²ä¸¦å¯«å…¥ Google Sheet (å·¥ä½œè¡¨4)\n",
        "    df_articles, crawl_status = crawl_ptt_koreastar(pages_to_fetch)\n",
        "\n",
        "    if df_articles.empty:\n",
        "        return (pd.DataFrame(), crawl_status, \"åˆ†æå¤±æ•—\", \"åˆ†æå¤±æ•—\", \"åˆ†æå¤±æ•—\")\n",
        "\n",
        "    # æ­¥é©Ÿ 2: åŸ·è¡Œ TF-IDF é—œéµå­—çµ±è¨ˆ (ä½¿ç”¨çˆ¬å–åˆ°çš„ DataFrame)\n",
        "    keyword_df, analyze_status = analyze_keywords(df_articles, top_n)\n",
        "\n",
        "    # æ­¥é©Ÿ 3: ä¸²æ¥ Gemini API ç”Ÿæˆæ‘˜è¦èˆ‡çµè«–\n",
        "    insight_output, conclusion_output = generate_summary(keyword_df, df_articles, user_question)\n",
        "\n",
        "    # æ–°å¢æ­¥é©Ÿï¼šå¯«å…¥åˆ†æçµæœåˆ°æ–°çš„ Google Sheet åˆ†é  (kpopç†±è©èˆ‡aiåˆ†æ)\n",
        "    gsheet_analysis_status = write_analysis_to_gsheet(keyword_df, insight_output, conclusion_output)\n",
        "\n",
        "    keyword_markdown = keyword_df.to_markdown(index=False)\n",
        "\n",
        "    # æ›´æ–°æœ€çµ‚ç‹€æ…‹ï¼ŒåŠ å…¥åˆ†æçµæœçš„å¯«å…¥ç‹€æ…‹\n",
        "    final_status = f\"{crawl_status}\\n{analyze_status}\\n{gsheet_analysis_status}\"\n",
        "\n",
        "    return (df_articles, final_status, keyword_markdown, insight_output, conclusion_output)\n",
        "\n",
        "def ask_question_workflow(question):\n",
        "    \"\"\"å•ç­”æ©Ÿå™¨äººæµç¨‹ï¼šç›´æ¥ä½¿ç”¨ Gemini API å›ç­”å•é¡Œ\"\"\"\n",
        "    if not question:\n",
        "        return \"è«‹è¼¸å…¥æ‚¨çš„å•é¡Œã€‚\"\n",
        "\n",
        "    # å¦‚æœ API Key éºå¤±ï¼Œç›´æ¥è¿”å›éŒ¯èª¤\n",
        "    if not API_KEY:\n",
        "        return \"âŒ éŒ¯èª¤ï¼šGemini API Key å°šæœªè¨­ç½®æˆ–ç„¡æ•ˆã€‚\"\n",
        "\n",
        "    system_prompt = (\n",
        "        \"ä½ æ˜¯ä¸€ä½å°ˆé–€å›ç­”éŸ“åœ‹å¨›æ¨‚åœˆã€K-Pop ç›¸é—œå•é¡Œçš„å°ˆå®¶ã€‚ \"\n",
        "        \"è«‹ä»¥æµæš¢çš„ç¹é«”ä¸­æ–‡ï¼Œè¦ªåˆ‡åœ°å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚ \"\n",
        "        \"å¦‚æœç­”æ¡ˆéœ€è¦æœ€æ–°è³‡è¨Šï¼Œè«‹ä½¿ç”¨æœå°‹å·¥å…·ä¾†è¼”åŠ©å›ç­”ã€‚\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            GEMINI_API_URL,\n",
        "            headers={'Content-Type': 'application/json'},\n",
        "            data=json.dumps({\n",
        "                \"contents\": [{\"parts\": [{\"text\": question}]}],\n",
        "                \"systemInstruction\": {\"parts\": [{\"text\": system_prompt}]},\\\n",
        "                \"tools\": [{\"google_search\": {}}]\n",
        "            }),\n",
        "            timeout=60\n",
        "        )\n",
        "\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "\n",
        "        generated_text = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', 'API è¿”å›ç©ºçµæœã€‚')\n",
        "\n",
        "        sources_text = \"\"\n",
        "        grounding_metadata = result.get('candidates', [{}])[0].get('groundingMetadata')\n",
        "        if grounding_metadata and grounding_metadata.get('groundingAttributions'):\n",
        "            sources = []\n",
        "            for attr in grounding_metadata['groundingAttributions']:\n",
        "                web = attr.get('web')\n",
        "                if web and web.get('title') and web.get('uri'):\n",
        "                    sources.append(f\"[{web['title']}]({web['uri']})\")\n",
        "\n",
        "            if sources:\n",
        "                sources_text = \"\\n\\n---\\n**åƒè€ƒä¾†æº:**\\n\" + \"\\n\".join(sources[:5])\n",
        "\n",
        "        return generated_text + sources_text\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        return f\"âŒ Gemini API å‘¼å«å¤±æ•—: {e}ã€‚è«‹æª¢æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆæˆ–æ˜¯å¦æœ‰è¶³å¤ çš„é¤˜é¡ã€‚\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"âŒ Gemini API å‘¼å«å¤±æ•—: {e}\"\n",
        "    except Exception as e:\n",
        "        return f\"è™•ç† Gemini éŸ¿æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\""
      ],
      "metadata": {
        "id": "qj1DNXmslRR4"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Gradio ä»‹é¢æ§‹å»º (ä¿®æ­£çµè«–é¡¯ç¤ºå’Œå€éš”æ–¹å¼) ---\n",
        "\n",
        "with gr.Blocks(title=\"PTT éŸ“æ˜Ÿè¶¨å‹¢åˆ†æç³»çµ±\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # PTT éŸ“æ˜Ÿç‰ˆ (KoreaStar) è¶¨å‹¢åˆ†æç³»çµ± ğŸ‡°ğŸ‡·âœ¨\n",
        "        **é‹è¡Œæ­¥é©Ÿï¼š**\n",
        "        1. åœ¨ä¸‹æ–¹è¨­å®šçˆ¬å–é æ•¸èˆ‡å•é¡Œï¼Œç„¶å¾Œé»æ“ŠåŸ·è¡Œã€‚\n",
        "        2. çˆ¬å–çµæœå°‡å¯«å…¥æ‚¨çš„ Google Sheet **çˆ¬èŸ²çµæœ**ã€‚\n",
        "        3. AI åˆ†æçµæœå°‡å¯«å…¥ Google Sheet **kpopç†±è©èˆ‡aiåˆ†æ**ã€‚\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Tab(\"éŸ“æ˜Ÿè¶¨å‹¢åˆ†æ (ä¸€éµåŸ·è¡Œ)\"):\n",
        "        gr.Markdown(\"## ğŸ“ˆ è¶¨å‹¢åˆ†æèˆ‡æ´å¯Ÿç”Ÿæˆ\")\n",
        "\n",
        "        with gr.Row():\n",
        "            pages_slider = gr.Slider(\n",
        "                minimum=1, maximum=30, value=10, step=1, label=\"è¦çˆ¬å–çš„ PTT é æ•¸\",\n",
        "                info=\"çˆ¬å–è¶Šå¤šé ï¼Œåˆ†æçµæœè¶Šå…¨é¢ï¼Œä½†è€—æ™‚è¶Šä¹…ã€‚å»ºè­°å¾ 10 é é–‹å§‹ã€‚\",\n",
        "                interactive=True\n",
        "            )\n",
        "            top_n_slider = gr.Slider(\n",
        "                minimum=5, maximum=20, value=10, step=1, label=\"Top N ç†±è©\",\n",
        "                info=\"TF-IDF çµ±è¨ˆè¦è¼¸å‡ºçš„ç†±è©æ•¸é‡ã€‚\",\n",
        "                interactive=True\n",
        "            )\n",
        "\n",
        "        user_input = gr.Textbox(\n",
        "            label=\"æ‚¨ç›®å‰æœ€é—œå¿ƒçš„éŸ“åœ‹å¨›æ¨‚åœˆå•é¡Œ/åœ˜é«”æ˜¯ä»€éº¼ï¼Ÿ\",\n",
        "            placeholder=\"ä¾‹å¦‚ï¼šæœ€è¿‘ NewJeans çš„å›æ­¸è¨è«–åº¦å¦‚ä½•ï¼Ÿ (æ­¤å•é¡Œæœƒæä¾›çµ¦ AI åƒè€ƒ)\",\n",
        "            lines=2\n",
        "        )\n",
        "\n",
        "        run_button = gr.Button(\"ğŸš€ ä¸€éµåŸ·è¡Œè¶¨å‹¢åˆ†æèˆ‡ Gemini ç¸½çµ\", variant=\"primary\")\n",
        "\n",
        "        status_text = gr.Markdown(\"--- åŸ·è¡Œç‹€æ…‹ ---\", elem_id=\"status_output\")\n",
        "\n",
        "        gr.Markdown(\"## ğŸ“‹ åˆ†æçµæœ\")\n",
        "\n",
        "        keyword_output = gr.Markdown(\n",
        "            label=\"TF-IDF ç†±è©çµ±è¨ˆ\",\n",
        "            value=\"ç­‰å¾…åŸ·è¡Œ...\"\n",
        "        )\n",
        "\n",
        "        data_preview = gr.DataFrame(\n",
        "            label=\"çˆ¬èŸ²æ•¸æ“šé è¦½ (å·²å¯«å…¥ Google Sheet 'çˆ¬èŸ²çµæœ' çš„å…§å®¹)\",\n",
        "            wrap=True\n",
        "        )\n",
        "\n",
        "        # ğŸš¨ ä¿®æ­£å€éš”æ–¹å¼èˆ‡æ¨™é¡Œå¤§å°\n",
        "        gr.Markdown(\"## ğŸ§  Gemini AI æ´å¯Ÿ\") # ä¿æŒä¸€å€‹å¤§æ¨™é¡Œçµ±ç¨±\n",
        "\n",
        "        insight_output = gr.Markdown(\n",
        "            label=\"5 å¥æ´å¯Ÿæ‘˜è¦\",\n",
        "            value=\"* ç­‰å¾…åŸ·è¡Œ...\"\n",
        "        )\n",
        "\n",
        "        # ğŸš¨ åªç”¨ä¸€æ¢æ°´å¹³ç·šå€éš”\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "        # ğŸš¨ ä½¿ç”¨è¼ƒå°çš„ Markdown æ¨™é¡Œ (###) ä½œç‚ºçµè«–çš„æ¨™ç±¤\n",
        "        gr.Markdown(\"### 120 å­—çµè«– (éŸ“åœ‹å¨›æ¨‚åœˆè¿‘æœŸåœ¨ç´…ä»€éº¼ï¼Ÿ)\")\n",
        "\n",
        "        conclusion_output = gr.Markdown(\n",
        "            label=\"çµè«–å…§æ–‡\", # å°‡ label ç§»åˆ°ä¸Šé¢çš„ Markdown\n",
        "            value=\"ç­‰å¾…åŸ·è¡Œ...\",\n",
        "            show_label=False # éš±è—å…ƒä»¶æœ¬èº«çš„ labelï¼Œåªé¡¯ç¤ºä¸Šæ–¹çš„ Markdown æ¨™é¡Œ\n",
        "        )\n",
        "        # -----------------------------\n",
        "\n",
        "        run_button.click(\n",
        "            fn=full_analysis_workflow,\n",
        "            inputs=[pages_slider, user_input, top_n_slider],\n",
        "            outputs=[data_preview, status_text, keyword_output, insight_output, conclusion_output]\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"AI å•ç­”æ©Ÿå™¨äºº\"):\n",
        "        gr.Markdown(\"## ğŸ¤– éŸ“åœ‹å¨›æ¨‚åœˆå•ç­”æ©Ÿå™¨äºº\")\n",
        "\n",
        "        question_input = gr.Textbox(\n",
        "            label=\"è«‹è¼¸å…¥æ‚¨å°éŸ“åœ‹å¨›æ¨‚åœˆå¥½å¥‡çš„å•é¡Œï¼š\",\n",
        "            placeholder=\"ä¾‹å¦‚ï¼šæœ€è¿‘æœ‰å“ªäº› K-POP åœ˜é«”ç™¼è¡Œäº†æ–°æ­Œï¼Ÿ\"\n",
        "        )\n",
        "        ask_button = gr.Button(\"æå• (ä¸²æ¥ Gemini API æœå°‹æœ€æ–°è³‡è¨Š)\", variant=\"secondary\")\n",
        "\n",
        "        answer_output = gr.Markdown(label=\"AI å›ç­”\", value=\"ç­‰å¾…æå•...\")\n",
        "\n",
        "        ask_button.click(\n",
        "            fn=ask_question_workflow,\n",
        "            inputs=[question_input],\n",
        "            outputs=[answer_output]\n",
        "        )\n",
        "\n",
        "# å•Ÿå‹• Gradio æ‡‰ç”¨\n",
        "if __name__ == \"__main__\":\n",
        "    jieba.initialize()\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "oMI9aSYStZxS",
        "outputId": "1db94dd2-1498-4b86-9775-e21fd17296e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://17e1bf78c34820f8e1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://17e1bf78c34820f8e1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}